# LLM Evaluation

This repository is a part of a larger project in trustworthy and explainable atrificial intelligence (AI, XAI), natural language processing (NLP) and large language models (LLMs).

The purpose of this code is to evaluate the performance of different LLMs in the context of instruction-based conversational interfaces.

The evaluation is performed by using "gold parse" datasets which contain a natural language utterance and corresponding parsed instruction(s).

We evaluate the models based on their accuracy of parsing natural language utterances into instruction(s) based on the "gold parse" datasets.